---
title: "Course-project-PML"
author: "Clara Perez"
date: "17/8/2020"
output: html_document
---

## Installing libraries

```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(e1071)
library(gbm)
```

## Data Loading

```{r}
setwd('C:/Users/ceper/Documents/Personales/Self learning/Statistics and Machine learning mini progra/3-Practical Machine Learning')
training <- read.csv('./pml-training.csv', header=T, na.strings=c("NA","#DIV/0!",""))
testing <- read.csv('./pml-testing.csv', header=T, na.strings=c("NA","#DIV/0!",""))
dim(training)
dim(testing)
```

## Partioning the training set into two

```{r}
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

## Cleaning the data

We start by removing NearZeroVariance variables

```{r}
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
```

Now, remove the first identification column of the myTraining data set and clean variables with more than 60% NA

```{r}
myTraining <- myTraining[c(-1)]

trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}
# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
```

Finally, we transform the myTesting and testing data sets

```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining
dim(myTesting)
dim(testing)
```

And coerce the data into the same type

```{r}
for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]
```

## Model building

For this project we will analyse three different models, in order to choose that with higher accuracy:

1. Classification trees 
2. Random forests
3. Generalized Boosted Model

### Classification trees
```{r}
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
```

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
```

```{r}
plot(cmtree$table, col = cmtree$byClass, 
     main = paste("Decision Tree Confusion Matrix: Accuracy =",
                  round(cmtree$overall['Accuracy'], 4)))
```

### Random Forests
```{r}
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
```

```{r}
plot(modFitB1)
```

```{r}
plot(cmrf$table, col = cmtree$byClass, 
     main = paste("Random Forest Confusion Matrix: Accuracy =",
                  round(cmrf$overall['Accuracy'], 4)))
```

## Prediction with Generalized Boosted 

## Regression

```{r}
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)
gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)
gbmFinMod1 <- gbmFit1$finalModel
gbmPredTest <- predict(gbmFit1, newdata=myTesting)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmAccuracyTest
```

```{r}
plot(gbmFit1, ylim = c(0.9,1))
```

## Predicting Results on the Test Data

Out of the three models, Random Forests gave the highest Accuracy = 99.89%. Compared with the Decision Trees (97.43%) or GBM (99.69%). 

With this result, the expected out-of-sample error is 100-99.89 = 0.11%; and the predictions would be as follows:

```{r}
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2
```

